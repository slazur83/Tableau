{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piICG08Xz8Ro"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "\n",
        "if not os.path.exists(drive_path):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "config_file_path = os.path.join(drive_path, \"Skrypty\", \"config.json\")\n",
        "\n",
        "try:\n",
        "    with open(config_file_path, 'r') as config_file:\n",
        "        config = json.load(config_file)\n",
        "        print(\"Configuration file loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Configuration file not found at {config_file_path}.\")\n",
        "except json.JSONDecodeError:\n",
        "    print(\"Error: Failed to parse the configuration file. Please check the file's content.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S4ouSJ5a_QAd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "SPECIAL_CHARACTER_MAPPING = {\n",
        "    \"Chylinska\": \"Chylińska\",\n",
        "    \"Toure\": \"Touré\",\n",
        "    \"Ashford Simpson\": \"Ashford & Simpson\",\n",
        "    \"Anderson Paak\": \"Anderson .Paak\",\n",
        "    \"Anderson .paak\": \"Anderson .Paak\",\n",
        "    \"Axwell Ingrosso\": \"Axwell /\\ Ingrosso\",\n",
        "    \"Zodiac Mindwarp The Love Reaction\": \"Zodiac Mindwarp & The Love Reaction\"\n",
        "}\n",
        "\n",
        "def standardize_artist_and_track(artist, track):\n",
        "\n",
        "    if not pd.notna(artist) or not pd.notna(track):\n",
        "        return artist, track\n",
        "\n",
        "    if artist:\n",
        "        artist = ' '.join(artist.split())\n",
        "        artist = artist.replace('St ', 'St. ').replace(' And ', ' & ').replace(' and ', ' & ').replace(' AND ', ' & ')\n",
        "        artist = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', artist)\n",
        "\n",
        "        for original, replacement in SPECIAL_CHARACTER_MAPPING.items():\n",
        "            artist = artist.replace(original, replacement)\n",
        "\n",
        "        if artist.lower().endswith(' vevo'):\n",
        "            artist = artist[:-5].strip()\n",
        "\n",
        "        words = artist.split()\n",
        "        artist = ' '.join(word.capitalize() if word.lower() != '&' else '&' for word in words)\n",
        "\n",
        "    if track:\n",
        "        track = ' '.join(track.strip().title().split())\n",
        "\n",
        "    feat_match = re.search(r'\\(feat\\. (.*?)\\)', track, re.IGNORECASE)\n",
        "    if feat_match:\n",
        "        featured_artist = feat_match.group(1).strip()\n",
        "        if featured_artist.lower() not in artist.lower():\n",
        "            artist = f\"{artist}, {featured_artist}\"\n",
        "        track = re.sub(r'\\(feat\\. .*?\\)', '', track, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    return artist, track\n",
        "\n",
        "\n",
        "def remove_duplicates(df, subset_columns):\n",
        "\n",
        "    before_count = df.shape[0]\n",
        "    print(f\"Before removing duplicates: {before_count} rows\")\n",
        "\n",
        "    df_cleaned = df.drop_duplicates(subset=subset_columns)\n",
        "\n",
        "    after_count = df_cleaned.shape[0]\n",
        "    print(f\"After removing duplicates: {after_count} rows\")\n",
        "\n",
        "    removed_count = before_count - after_count\n",
        "    print(f\"Number of duplicates removed: {removed_count} rows\")\n",
        "\n",
        "    return df_cleaned\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYt3e2Mrej3L"
      },
      "source": [
        "# **LastFM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPxi0Ga4SyZo"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import csv\n",
        "import shutil\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "last_fm_api_key = config['last_fm_api_key']\n",
        "user_name = 'slazur83'\n",
        "\n",
        "base_url = \"http://ws.audioscrobbler.com/2.0/\"\n",
        "params = {\n",
        "    'method': 'user.getrecenttracks',\n",
        "    'user': user_name,\n",
        "    'api_key': last_fm_api_key,\n",
        "    'format': 'json',\n",
        "    'limit': 200,\n",
        "    'page': 1\n",
        "}\n",
        "\n",
        "base_filename = \"lastfm_tracks.csv\"\n",
        "directory = '.'\n",
        "drive_folder = '/content/drive/MyDrive/Dane z aplikacji/LastFM/'\n",
        "\n",
        "def format_date(date_str):\n",
        "    if date_str == 'N/A':\n",
        "        return 'N/A'\n",
        "\n",
        "    try:\n",
        "        return datetime.strptime(date_str, '%d %b %Y, %H:%M:%S').strftime('%Y-%m-%d %H:%M')\n",
        "    except ValueError:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, '%d %b %Y, %H:%M').strftime('%Y-%m-%d %H:%M')\n",
        "        except ValueError:\n",
        "            print(f\"Date format error for: {date_str}\")\n",
        "            return 'Invalid Date Format'\n",
        "\n",
        "def convert_to_datetime(date_str):\n",
        "    try:\n",
        "        return pd.to_datetime(date_str, format='%Y-%m-%d %H:%M', errors='coerce')\n",
        "    except (ValueError, TypeError) as e:\n",
        "        print(f\"Error converting {date_str}: {e}\")\n",
        "        return pd.NaT\n",
        "\n",
        "def get_recent_tracks(params, max_retries=5):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            response = requests.get(base_url, params=params)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.HTTPError as http_err:\n",
        "            if response.status_code >= 500:\n",
        "                retries += 1\n",
        "                wait_time = 2 ** retries\n",
        "                print(f\"Server error {response.status_code}. Retrying in {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"HTTP error occurred: {http_err}\")\n",
        "                raise\n",
        "        except requests.exceptions.RequestException as err:\n",
        "            print(f\"Error during requests: {err}\")\n",
        "            raise\n",
        "    print(f\"Failed after {max_retries} retries.\")\n",
        "    return None\n",
        "\n",
        "csv_file = base_filename\n",
        "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['artist', 'track', 'album', 'playback_date'])\n",
        "\n",
        "    while True:\n",
        "        data = get_recent_tracks(params)\n",
        "        if not data:\n",
        "            break\n",
        "\n",
        "        tracks = data.get('recenttracks', {}).get('track', [])\n",
        "        if not tracks:\n",
        "            break\n",
        "\n",
        "        for track in tracks:\n",
        "            artist = track.get('artist', {}).get('#text', 'N/A')\n",
        "            track_name = track.get('name', 'N/A')\n",
        "            album = track.get('album', {}).get('#text', 'N/A')\n",
        "            playback_date = format_date(track.get('date', {}).get('#text', 'N/A'))\n",
        "\n",
        "            writer.writerow([artist, track_name, album, playback_date])\n",
        "\n",
        "        params['page'] += 1\n",
        "\n",
        "print(f\"Data written to {csv_file}\")\n",
        "\n",
        "shutil.copy(csv_file, drive_folder)\n",
        "print(f\"The file {csv_file} has been moved to {drive_folder}.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_lastfm = pd.read_csv(drive_folder + csv_file, header=0)\n",
        "df_lastfm.columns = ['Artist', 'Track', 'Album', 'Date']\n",
        "\n",
        "df_lastfm['Date'] = df_lastfm['Date'].apply(convert_to_datetime)\n",
        "df_lastfm['Date'] = df_lastfm['Date'].dt.strftime('%Y-%m-%d %H:%M')\n",
        "\n",
        "df_lastfm['Source'] = 'LastFM'\n",
        "df_lastfm['Account'] = 'slazur83'\n",
        "\n",
        "df_lastfm[['Artist', 'Track']] = df_lastfm.apply(lambda row: standardize_artist_and_track(row['Artist'], row['Track']), axis=1, result_type=\"expand\")\n",
        "\n",
        "df_lastfm = remove_duplicates(df_lastfm, ['Date', 'Artist', 'Track'])"
      ],
      "metadata": {
        "id": "qW9455axpDfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN45KgJ9Dc-w"
      },
      "source": [
        "# **YouTube Music**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tabulate import tabulate\n",
        "\n",
        "folder_paths = {\n",
        "    'riwanna85': '/content/drive/MyDrive/Dane z aplikacji/Google/riwanna85/YouTube i YouTube Music/historia/',\n",
        "    'slazur83': '/content/drive/MyDrive/Dane z aplikacji/Google/slazur83/YouTube i YouTube Music/historia/'\n",
        "}\n",
        "file_name = 'historia oglądania.json'\n",
        "\n",
        "def extract_artist(subtitles):\n",
        "    return subtitles[0].get('name', '').split(' - ')[0] if isinstance(subtitles, list) and subtitles else ''\n",
        "\n",
        "def extract_song_title(title):\n",
        "    return title.replace(\"Obejrzano: \", \"\") if title.startswith(\"Obejrzano: \") else title\n",
        "\n",
        "def load_and_process_data(folder_path, account_name):\n",
        "    source_file = os.path.join(folder_path, file_name)\n",
        "    if not os.path.isfile(source_file):\n",
        "        print(f\"Brak pliku historii: {source_file}\")\n",
        "        return pd.DataFrame(columns=['Artist', 'Track', 'Date', 'Duration', 'Source', 'Account'])\n",
        "\n",
        "    df = pd.read_json(source_file, encoding='utf-8')\n",
        "    df = df[df['header'] == 'YouTube Music'].copy()\n",
        "    df['Artist'] = df['subtitles'].apply(extract_artist)\n",
        "    df['Track'] = df['title'].apply(extract_song_title)\n",
        "    df['Date'] = pd.to_datetime(df['time'], format='ISO8601').dt.strftime('%Y-%m-%d %H:%M')\n",
        "    df['Source'], df['Account'], df['Duration'] = 'YouTube Music', account_name, 'N/A'\n",
        "    return df[['Artist', 'Track', 'Date', 'Duration', 'Source', 'Account']]\n",
        "\n",
        "df_ytmusic = pd.concat(\n",
        "    [load_and_process_data(path, account) for account, path in folder_paths.items()],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "df_ytmusic[['Artist', 'Track']] = df_ytmusic.apply(\n",
        "    lambda row: standardize_artist_and_track(row['Artist'], row['Track']), axis=1, result_type=\"expand\"\n",
        ")\n",
        "\n",
        "df_ytmusic = remove_duplicates(df_ytmusic, ['Date', 'Artist', 'Track'])"
      ],
      "metadata": {
        "id": "hDKuOh62ITsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_account_data(account_data):\n",
        "    return account_data.groupby('Account').agg(\n",
        "        start_date=('Date', 'min'), end_date=('Date', 'max'), row_count=('Account', 'size')\n",
        "    ).reset_index()\n",
        "\n",
        "final_summary = summarize_account_data(df_ytmusic)\n",
        "\n",
        "print(\"\\nFinal Summary of Accounts\")\n",
        "print(tabulate(final_summary, headers='keys', tablefmt='pretty'))\n",
        "\n",
        "overall_start_date, overall_end_date, overall_row_count = (\n",
        "    df_ytmusic['Date'].min(), df_ytmusic['Date'].max(), df_ytmusic.shape[0]\n",
        ") if not df_ytmusic.empty else (None, None, 0)\n",
        "\n",
        "print(f\"\\nOverall Summary:\\nDate range: from {overall_start_date} to {overall_end_date}\\nTotal Row count: {overall_row_count}\\n\")"
      ],
      "metadata": {
        "id": "kKxXVdAmLik4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqLYlxpjTaKw"
      },
      "source": [
        "# **Spotify**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QPutDy0IGH6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import logging\n",
        "from tabulate import tabulate\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def load_spotify_data(base_path, account_name):\n",
        "    mydata_folders = glob.glob(os.path.join(base_path, \"MyData*\"))\n",
        "    df_list = []\n",
        "\n",
        "    for folder in mydata_folders:\n",
        "        takeout_name = os.path.basename(folder)\n",
        "\n",
        "        all_files = glob.glob(os.path.join(folder, \"StreamingHistory*.json\")) + \\\n",
        "                    glob.glob(os.path.join(folder, \"Streaming_History_Audio_*.json\")) + \\\n",
        "                    glob.glob(os.path.join(folder, \"Streaming_History_Video_*.json\"))\n",
        "\n",
        "        music_files = {file for file in all_files if 'podcast' not in os.path.basename(file).lower() and 'video' not in os.path.basename(file).lower()}\n",
        "\n",
        "        logging.info(f\"\\nFiles found in {takeout_name} for account {account_name}:\")\n",
        "        for file in music_files:\n",
        "            logging.info(f\" - {file}\")\n",
        "\n",
        "            try:\n",
        "                with open(file, 'r', encoding='utf-8') as f:\n",
        "                    if f.read().strip():\n",
        "                        df = pd.read_json(file)\n",
        "\n",
        "                        if 'endTime' in df.columns:\n",
        "                            df = df.rename(columns={'endTime': 'Date', 'artistName': 'Artist', 'trackName': 'Track', 'msPlayed': 'Duration'})\n",
        "                        elif 'ts' in df.columns and 'master_metadata_track_name' in df.columns:\n",
        "                            df = df.rename(columns={'ts': 'Date', 'master_metadata_album_artist_name': 'Artist', 'master_metadata_track_name': 'Track', 'ms_played': 'Duration'})\n",
        "                            df['Date'] = pd.to_datetime(df['Date'], errors='coerce', utc=True)\n",
        "                            df['Date'] = df['Date'].dt.floor('min')\n",
        "\n",
        "                        df['Account'] = account_name\n",
        "                        df['Takeout'] = takeout_name\n",
        "                        df_list.append(df[['Artist', 'Track', 'Date', 'Duration', 'Account', 'Takeout']])\n",
        "                    else:\n",
        "                        logging.warning(f\"Skipping empty file: {file}\")\n",
        "            except ValueError as e:\n",
        "                logging.error(f\"Error reading {file}: {e} - Skipping this file.\")\n",
        "\n",
        "    if df_list:\n",
        "        combined_df = pd.concat(df_list, ignore_index=True)\n",
        "    else:\n",
        "        combined_df = pd.DataFrame()\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "df1 = load_spotify_data('/content/drive/MyDrive/Dane z aplikacji/Spotify/slazur83@gmail.com/', 'slazur83')\n",
        "df2 = load_spotify_data('/content/drive/MyDrive/Dane z aplikacji/Spotify/zethar182@gmail.com/', 'zethar182')\n",
        "\n",
        "df_spotify = pd.concat([df1, df2], ignore_index=True)\n",
        "df_spotify['Source'] = 'Spotify'\n",
        "\n",
        "df_spotify['Date'] = pd.to_datetime(df_spotify['Date'], errors='coerce', utc=True)\n",
        "df_spotify = df_spotify.dropna(subset=['Date'])\n",
        "\n",
        "df_spotify['Date'] = df_spotify['Date'].dt.strftime('%Y-%m-%d %H:%M')\n",
        "\n",
        "df_spotify[['Artist', 'Track']] = df_spotify.apply(\n",
        "    lambda row: standardize_artist_and_track(row['Artist'], row['Track']), axis=1, result_type=\"expand\"\n",
        ")\n",
        "\n",
        "df_spotify = remove_duplicates(df_spotify, ['Date', 'Artist', 'Track'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_account_data(account_data):\n",
        "    takeout_summary = account_data.groupby('Takeout').agg(\n",
        "        start_date=('Date', 'min'),\n",
        "        end_date=('Date', 'max'),\n",
        "        row_count=('Takeout', 'count')\n",
        "    ).reset_index()\n",
        "\n",
        "    takeout_summary['Account'] = account_data['Account'].iloc[0]\n",
        "    return takeout_summary[['Account', 'Takeout', 'start_date', 'end_date', 'row_count']]\n",
        "\n",
        "final_summary = pd.concat([\n",
        "    summarize_account_data(df_spotify[df_spotify['Account'] == 'zethar182']),\n",
        "    summarize_account_data(df_spotify[df_spotify['Account'] == 'slazur83'])\n",
        "], ignore_index=True)\n",
        "\n",
        "print(\"\\nFinal Summary of Accounts\")\n",
        "print(tabulate(final_summary, headers='keys', tablefmt='pretty'))\n",
        "\n",
        "overall_start_date = df_spotify['Date'].min() if not df_spotify.empty else None\n",
        "overall_end_date = df_spotify['Date'].max() if not df_spotify.empty else None\n",
        "overall_row_count = df_spotify.shape[0] if not df_spotify.empty else 0\n",
        "\n",
        "print(f\"\\nOverall Summary:\\nDate range: from {overall_start_date} to {overall_end_date}\\nTotal Row count: {overall_row_count}\\n\")"
      ],
      "metadata": {
        "id": "EvMtOqkuL3-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8veMxiBg5QJ"
      },
      "source": [
        "# **Deezer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xH9JpEB3SVt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "deezer_file = '/content/drive/MyDrive/Dane z aplikacji/Deezer/4519420622.xlsx'\n",
        "\n",
        "df_deezer = pd.read_excel(deezer_file, sheet_name=\"10_listeningHistory\")\n",
        "\n",
        "df_deezer = df_deezer.rename(columns={\n",
        "    'Song Title': 'Track',\n",
        "    'Album Title': 'Album',\n",
        "    'Listening Time': 'Duration',\n",
        "    'Platform Name': 'Platform'\n",
        "})\n",
        "\n",
        "df_deezer = df_deezer.drop(columns=['ISRC'], errors='ignore')\n",
        "\n",
        "df_deezer['Source'] = 'Deezer'\n",
        "df_deezer['Account'] = 'slazur83'\n",
        "\n",
        "df_deezer['Date'] = pd.to_datetime(df_deezer['Date'], errors='coerce')\n",
        "df_deezer['Date'] = df_deezer['Date'].dt.strftime('%Y-%m-%d %H:%M')\n",
        "\n",
        "df_deezer[['Artist', 'Track']] = df_deezer.apply(\n",
        "    lambda row: standardize_artist_and_track(row['Artist'], row['Track']), axis=1, result_type=\"expand\"\n",
        ")\n",
        "\n",
        "df_deezer = df_deezer.dropna(subset=['Date'])\n",
        "\n",
        "df_deezer = remove_duplicates(df_deezer, ['Date', 'Artist', 'Track'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEZDcOfTLZD7"
      },
      "source": [
        "**Final DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1hRVXB7Ntg5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "merged = pd.concat([df_spotify, df_ytmusic, df_lastfm, df_deezer], ignore_index=True)\n",
        "\n",
        "merged['Date'] = pd.to_datetime(merged['Date']).dt.strftime('%Y-%m-%d %H:%M')\n",
        "\n",
        "total_length = len(df_spotify) + len(df_ytmusic) + len(df_lastfm) + len(df_deezer)\n",
        "assert len(merged) == total_length, \"Not all data is merged\"\n",
        "print('All data is merged')\n",
        "\n",
        "nulls = merged['Date'].isna().sum()\n",
        "if nulls > 0:\n",
        "    if nulls < 15:\n",
        "        merged.dropna(subset=['Date'], inplace=True)\n",
        "        print(f'Dropped {nulls} rows with missing Date values.')\n",
        "    else:\n",
        "        raise ValueError(f'There are {nulls} null values in the Date column that need attention.')\n",
        "else:\n",
        "    print('No null values found in the Date column.\\n')\n",
        "\n",
        "columns_order = ['Date', 'Artist', 'Track', 'Album', 'Duration', 'Source', 'Sub-source', 'Account', 'Platform', 'Platform Model', 'IP Address']\n",
        "merged = merged.reindex(columns=columns_order).copy()\n",
        "\n",
        "merged['Date'] = pd.to_datetime(merged['Date'])\n",
        "merged = merged.sort_values(by='Date', inplace=False)\n",
        "\n",
        "lastfm_data = merged[merged['Source'] == 'LastFM'].copy()\n",
        "other_sources_data = merged[merged['Source'] != 'LastFM'].copy()\n",
        "\n",
        "other_sources_data['Sub-source'] = other_sources_data['Source']\n",
        "\n",
        "unmatched_lastfm = lastfm_data.merge(\n",
        "    other_sources_data,\n",
        "    on=['Artist', 'Track'],\n",
        "    how='left',\n",
        "    suffixes=('', '_merged'),\n",
        "    indicator=True\n",
        ")\n",
        "\n",
        "unmatched_lastfm = unmatched_lastfm[\n",
        "    (unmatched_lastfm['_merge'] == 'left_only') |\n",
        "    ((unmatched_lastfm['_merge'] == 'both') &\n",
        "     ((abs(unmatched_lastfm['Date'] - unmatched_lastfm['Date_merged']).dt.total_seconds() / 60) > 10))\n",
        "]\n",
        "\n",
        "unmatched_lastfm = unmatched_lastfm[['Date', 'Artist', 'Track', 'Source', 'Sub-source']]\n",
        "unmatched_lastfm = unmatched_lastfm.copy()\n",
        "unmatched_lastfm['Sub-source'] = 'N/A'\n",
        "unmatched_lastfm['Sub-source'] = unmatched_lastfm['Sub-source'].astype(object)\n",
        "\n",
        "all_merged = pd.concat([other_sources_data, unmatched_lastfm], ignore_index=True)\n",
        "all_merged = all_merged.drop_duplicates(subset=['Date', 'Artist', 'Track'])\n",
        "all_merged.loc[:, 'Source'] = 'All Merged'\n",
        "\n",
        "merged['Sub-source'] = merged.get('Sub-source', 'N/A')\n",
        "\n",
        "final_df = pd.concat([merged, all_merged], ignore_index=True)\n",
        "final_df = final_df.reindex(columns=columns_order)\n",
        "\n",
        "for source in final_df['Source'].unique():\n",
        "    source_data = final_df[final_df['Source'] == source]\n",
        "\n",
        "    total_rows = source_data.shape[0]\n",
        "    start_date = source_data['Date'].min().strftime('%Y-%m-%d %H:%M') if total_rows > 0 else \"N/A\"\n",
        "    end_date = source_data['Date'].max().strftime('%Y-%m-%d %H:%M') if total_rows > 0 else \"N/A\"\n",
        "    distinct_artists = source_data['Artist'].nunique()\n",
        "    distinct_tracks = source_data['Track'].nunique()\n",
        "\n",
        "    print(f\"Source: {source}\")\n",
        "    print(f\"  Total rows: {total_rows}\")\n",
        "    print(f\"  Start date: {start_date}\")\n",
        "    print(f\"  End date: {end_date}\")\n",
        "    print(f\"  Distinct artists: {distinct_artists}\")\n",
        "    print(f\"  Distinct tracks: {distinct_tracks}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_path1 = '/content/drive/MyDrive/Skrypty/Tableau/Outputs/music_tracks.csv'\n",
        "output_path2 = '/content/drive/MyDrive/Skrypty/Tableau/Outputs/music_tracks.xlsx'\n",
        "\n",
        "final_df.to_csv(output_path1, index=False)\n",
        "print(f'Data successfully exported to {output_path1}')\n",
        "\n",
        "final_df.to_excel(output_path2, index=False)\n",
        "print(f'Data successfully exported to {output_path2}')"
      ],
      "metadata": {
        "id": "LhcpMz96nXDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NGi0yoJ2G"
      },
      "source": [
        "**Matching entries in LastFM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuCDlP7Ma1i0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "merged_data = final_df.copy()\n",
        "\n",
        "merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
        "merged_data['Duration'] = pd.to_numeric(merged_data['Duration'], errors='coerce')\n",
        "\n",
        "filtered_sources = merged_data.query(\"Duration.isna() or Duration >= 30\")\n",
        "\n",
        "lastfm_data = merged_data.query(\"Source == 'LastFM'\").copy()\n",
        "lastfm_data['key'] = lastfm_data[['Artist', 'Track']].apply(tuple, axis=1)\n",
        "\n",
        "def match_within_period(row, lastfm_data):\n",
        "    time_window = pd.Timedelta(minutes=5)\n",
        "    matching_rows = lastfm_data[\n",
        "        (lastfm_data['key'] == row['key']) &\n",
        "        (abs(lastfm_data['Date'] - row['Date']) <= time_window)\n",
        "    ]\n",
        "    return not matching_rows.empty\n",
        "\n",
        "report = []\n",
        "\n",
        "for source in merged_data['Source'].unique():\n",
        "    if source in ('LastFM', 'All Merged'):\n",
        "        continue\n",
        "\n",
        "    source_data = merged_data.query(\"Source == @source\")\n",
        "\n",
        "    start_date = lastfm_data['Date'].min()\n",
        "    end_date = source_data['Date'].max()\n",
        "\n",
        "    filtered_source_data = source_data.query(\"@start_date <= Date <= @end_date\").copy()\n",
        "    filtered_source_data['key'] = filtered_source_data[['Artist', 'Track']].apply(tuple, axis=1)\n",
        "\n",
        "    filtered_source_data['Exists_in_LastFM'] = filtered_source_data.apply(\n",
        "        match_within_period, lastfm_data=lastfm_data, axis=1\n",
        "    )\n",
        "\n",
        "    matching_count = filtered_source_data['Exists_in_LastFM'].sum()\n",
        "    total_source_count = len(filtered_source_data)\n",
        "    matching_percentage = (matching_count / total_source_count * 100) if total_source_count > 0 else 0\n",
        "\n",
        "    report.append({\n",
        "        'Source': source,\n",
        "        'Start Date': start_date.strftime(\"%Y-%m-%d\"),\n",
        "        'End Date': end_date.strftime(\"%Y-%m-%d\"),\n",
        "        'Total Entries': total_source_count,\n",
        "        'Matching Entries': matching_count,\n",
        "        'Matching Percentage': f\"{matching_percentage:.2f}%\"\n",
        "    })\n",
        "\n",
        "for entry in report:\n",
        "    print(f\"Source: {entry['Source']}\")\n",
        "    print(f\"  Start Date: {entry['Start Date']}\")\n",
        "    print(f\"  End Date: {entry['End Date']}\")\n",
        "    print(f\"  Total Entries: {entry['Total Entries']}\")\n",
        "    print(f\"  Matching Entries: {entry['Matching Entries']}\")\n",
        "    print(f\"  Percentage of Matching Entries in LastFM: {entry['Matching Percentage']}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WaAQZkYfcotm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "unique_artists = final_df[\"Artist\"].dropna().unique()\n",
        "unique_artists = [str(artist) for artist in unique_artists]\n",
        "\n",
        "unique_tracks = final_df[\"Track\"].dropna().unique()\n",
        "unique_tracks = [str(track) for track in unique_tracks]\n",
        "\n",
        "sorted_artists = sorted(unique_artists)\n",
        "sorted_tracks = sorted(unique_tracks)\n",
        "\n",
        "artist_track_df = final_df[[\"Artist\", \"Track\"]].dropna().drop_duplicates()\n",
        "artist_track_df = artist_track_df.sort_values(by=[\"Artist\", \"Track\"])\n",
        "\n",
        "artist_track_counts = final_df.groupby([\"Artist\", \"Track\"]).size().reset_index(name=\"Count\")\n",
        "artist_track_counts = artist_track_counts.sort_values(by=\"Count\", ascending=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Nxr7fSxPw5_9luzBYxemvGg8v40m-82q",
      "authorship_tag": "ABX9TyPztVNr2QsWTjffFKE2RkDx"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}